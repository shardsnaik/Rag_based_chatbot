# ğŸ¥ Medical Bot â€“ FastAPI + Quantized MedGemma (GGUF)

# https://conversationaimodel.netlify.app/

A production-ready medical conversational AI backend built using FastAPI, LangChain, and a 5-bit quantized MedGemma 4B GGUF model, designed to run efficiently on CPU-only environments such as AWS Lambda (container-based) or traditional cloud services.

This backend exposes REST APIs for chat, memory, and session management, and is intended to be consumed by a React frontend (Netlify) or any HTTP client.

## ğŸš€ Key Features

 * âœ… Quantized GGUF model (Q5_K_M) for low-memory CPU inference

* âœ… FastAPI backend with async support

* âœ… LangChain conversational memory (context-aware chat)

* âœ… Session-based chat history

* âœ… Hugging Face Hub integration for model download

* âœ… AWS Lambda compatible (uses /tmp storage + Mangum)

* âœ… CORS-enabled for browser clients (Netlify / React)

## ğŸ§  Model Details

* Base model: MedGemma 4B (instruction-tuned)

* Format: GGUF

* Quantization: Q5_K_M (CPU-friendly)

* Source: Hugging Face Hub

* Loaded via: llama-cpp-python

The model is **downloaded at startup** from Hugging Face and cached locally:

* ` /tmp/huggingface_cache (AWS Lambda)`

* ` ./model_cache (local / non-Lambda)`

## ğŸ—ï¸ Architecture Overview

```
React (Netlify)
      |
      |  HTTPS (POST /invocation)
      v
API Gateway
      |
      v
AWS Lambda (FastAPI + LangChain + llama.cpp)
      |
      v
Hugging Face Hub (GGUF model)
```

* The model and API live in the same FastAPI service

* The model is loaded once per container cold start

* Conversation memory is stored in-memory per session

## ğŸ“‚ Project Structure

```
medical_bot_main_file.py   # Main FastAPI + model + endpoints
.env                       # Environment variables (HF_TOKEN, etc.)
requirements.txt
README.md
```

## ğŸ”‘ Environment Variables

Create a .env file or configure in AWS Lambda:
```
HF_TOKEN=your_huggingface_token   # Only required for private models
```

## ğŸ”Œ API Endpoints
ğŸ  Root

``GET /``

Returns API status and available endpoints.

## ğŸ’¬ Chat (Main Endpoint)

``POST /invocation``

### Request
```
{
  "query": "What are the symptoms of diabetes?"
}
```

### Response
```
{
  "response": "Diabetes symptoms may include...",
  "status": "success"
}
```

## ğŸ§  Get Conversation Memory

`GET /memory`

Returns formatted chat history for the current session.

## ğŸ§¹ Clear Conversation Memory

`GET /clear-memory`

Clears the in-memory conversation history.

## ğŸ†” Get Session Info

`GET /get_session_id`

Returns session ID, uptime, and message count.

## â¤ï¸ Health Check

`GET /health`

Returns service health and model initialization status.

## ğŸŒ CORS Configuration

CORS is enabled to support browser clients:

```
allow_origins=["*"]
allow_methods=["GET", "POST", "OPTIONS"]
allow_headers=["*"]
```

âš ï¸ For production, restrict this to your Netlify domain.

## ğŸ§ª Local Development
### 1ï¸âƒ£ Install dependencies

```
pip install -r requirements.txt
```

### 2ï¸âƒ£ Run locally
python medical_bot_main_file.py


Server starts at:

```
http://127.0.0.1:8000
```

Swagger UI:

`http://127.0.0.1:8000/docs`

## â˜ï¸ AWS Lambda Deployment Notes

* Uses Mangum to adapt FastAPI â†’ Lambda

* Model stored in Hugging Face Hub, cached in /tmp

* Recommended deployment method:

  * Lambda Container Image (Docker)

* Memory: â‰¥ 6â€“8 GB

* Timeout: â‰¥ 60 seconds

âš ï¸ Cold start will include model download + load.

## âš ï¸ Medical Disclaimer

This system is for educational and informational purposes only.
It does not provide medical diagnosis or treatment advice.
Always consult a qualified healthcare professional.

## ğŸ“Œ Tech Stack

* FastAPI
* LangChain
* llama-cpp-python
* Hugging Face Hub
* AWS Lambda
* Mangum
* Python 3.10+

## âœ… Status

âœ” Production-ready
âœ” CPU-optimized
âœ” Frontend-compatible
âœ” Cloud-deployable

